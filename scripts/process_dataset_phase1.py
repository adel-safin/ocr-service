#!/usr/bin/env python3
"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –ø–∞—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ "–î–∞—Ç–∞—Å–µ—Ç" –ø–æ –§–∞–∑–µ 1
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–∞–ø–∫—É data
"""
import sys
from pathlib import Path
import json
from datetime import datetime

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –ø—É—Ç—å
app_root = Path(__file__).parent.parent
if str(app_root) not in sys.path:
    sys.path.insert(0, str(app_root))

from core.processor import DocumentPipeline
from utils.dataset_loader import DatasetLoader
import logging

logging.basicConfig(
    level=logging.WARNING,  # –£–º–µ–Ω—å—à–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def process_dataset_phase1():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –ø–∞—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –§–∞–∑–µ 1"""
    print("=" * 100)
    print("–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–¢–ê–°–ï–¢–ê - –§–ê–ó–ê 1")
    print("=" * 100)
    
    # –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
    dataset_root = Path("../–î–∞—Ç–∞—Å–µ—Ç")
    
    if not dataset_root.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {dataset_root}")
        return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    loader = DatasetLoader(str(dataset_root))
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –í–°–ï–• –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–µ —Ç–æ–ª—å–∫–æ –ø–∞—Ä
    documents_dir = dataset_root / "–ù–∞–±–æ—Ä—ã –æ–¥–Ω–æ—Ç–∏–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ —Å–∫–∞–Ω–∞–º–∏"
    
    if not documents_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {documents_dir}")
        return
    
    all_images = []
    doc_types = loader.get_all_document_types()
    
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_types)}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö –ø–∞–ø–æ–∫
    for doc_type_dir in documents_dir.iterdir():
        if not doc_type_dir.is_dir():
            continue
        
        doc_type_name = doc_type_dir.name
        images_in_type = []
        
        for file in doc_type_dir.iterdir():
            if file.is_file() and file.suffix.lower() in {'.pdf', '.jpg', '.jpeg', '.png', '.bmp'}:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —ç—Ç–∞–ª–æ–Ω
                reference_path = None
                base_name = file.stem
                
                # –ò—â–µ–º —ç—Ç–∞–ª–æ–Ω —Å –ø–æ—Ö–æ–∂–∏–º –∏–º–µ–Ω–µ–º
                for ref_file in doc_type_dir.iterdir():
                    if ref_file.is_file() and ref_file.suffix.lower() in {'.doc', '.docx', '.txt', '.xlsx'}:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –∏–º–µ–Ω–∏
                        ref_base = loader._get_base_name(ref_file.stem)
                        img_base = loader._get_base_name(base_name)
                        
                        if ref_base == img_base or ref_file.stem == base_name:
                            reference_path = str(ref_file)
                            break
                
                all_images.append({
                    'document_type': doc_type_name,
                    'image_path': str(file),
                    'reference_path': reference_path,
                    'has_reference': reference_path is not None
                })
                images_in_type.append(file.name)
        
        print(f"   {doc_type_name}: {len(images_in_type)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    print(f"\nüìÅ –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(all_images)}")
    print(f"   –° —ç—Ç–∞–ª–æ–Ω–∞–º–∏: {sum(1 for img in all_images if img['has_reference'])}")
    print(f"   –ë–µ–∑ —ç—Ç–∞–ª–æ–Ω–æ–≤: {sum(1 for img in all_images if not img['has_reference'])}")
    
    if not all_images:
        print("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return
    
    all_pairs = all_images
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –§–∞–∑—ã 1
    print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –§–∞–∑—ã 1...")
    pipeline = DocumentPipeline(use_ml=False, use_active_learning=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_dir = Path("data/dataset_results_phase1")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    results = []
    processed = 0
    errors = 0
    
    print(f"\n‚è≥ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏...\n")
    
    for i, pair in enumerate(all_pairs, 1):
        image_path = Path(pair['image_path'])
        doc_type = pair['document_type']
        
        print(f"[{i}/{len(all_pairs)}] {doc_type}: {image_path.name}")
        
        try:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            image_path_str = pair['image_path']
            result = pipeline.process(image_path_str, template=doc_type.lower().replace(' ', '_'))
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
            image_path_obj = Path(pair['image_path'])
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
            pages_data = result['extracted_data'].get('pages', [])
            total_pages = result['extracted_data'].get('total_pages', 1)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
            text_file = output_dir / f"{image_path_obj.stem}_phase1.txt"
            
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(f"–¢–ò–ü –î–û–ö–£–ú–ï–ù–¢–ê: {doc_type}\n")
                f.write(f"–§–ê–ô–õ: {image_path_obj.name}\n")
                f.write(f"Document ID: {result['document_id']}\n")
                f.write(f"–ö–∞—á–µ—Å—Ç–≤–æ: {result['quality_report']['overall_quality']:.2%}\n")
                f.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {result['quality_report']['ocr_confidence']:.2%}\n")
                f.write(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(result.get('corrections_applied', []))}\n")
                f.write(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}\n")
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
                if pair.get('has_reference') and pair.get('reference_path'):
                    try:
                        reference_text = loader.load_reference_text(pair['reference_path'])
                        if reference_text.strip():
                            f.write("\n" + "=" * 100 + "\n")
                            f.write("–≠–¢–ê–õ–û–ù–ù–´–ô –¢–ï–ö–°–¢:\n")
                            f.write("=" * 100 + "\n")
                            f.write(reference_text)
                            f.write("\n" + "=" * 100 + "\n")
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —ç—Ç–∞–ª–æ–Ω: {str(e)}")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
                if pages_data and len(pages_data) > 1:
                    f.write("\n" + "=" * 100 + "\n")
                    f.write("–†–ê–°–ü–û–ó–ù–ê–ù–ù–´–ô –¢–ï–ö–°–¢ –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú:\n")
                    f.write("=" * 100 + "\n")
                    for page_info in pages_data:
                        page_num = page_info.get('page_number', 1)
                        page_text = page_info.get('text', '')
                        page_conf = page_info.get('confidence', 0.0)
                        f.write(f"\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {page_conf:.2%}) ---\n")
                        f.write(page_text)
                        f.write("\n")
                else:
                    f.write("\n" + "=" * 100 + "\n")
                    f.write("–†–ê–°–ü–û–ó–ù–ê–ù–ù–´–ô –¢–ï–ö–°–¢:\n")
                    f.write("=" * 100 + "\n")
                    f.write(result['extracted_data']['full_text'])
                    f.write("\n" + "=" * 100 + "\n")
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö
                if result.get('corrections_applied'):
                    f.write("\n–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:\n")
                    f.write("-" * 100 + "\n")
                    for j, correction in enumerate(result['corrections_applied'], 1):
                        f.write(f"{j}. {correction.get('from', '')} -> {correction.get('to', '')}\n")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–µ—Å–ª–∏ –±–æ–ª—å—à–µ 1 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
            if pages_data and len(pages_data) > 1:
                pages_dir = output_dir / f"{image_path_obj.stem}_pages"
                pages_dir.mkdir(exist_ok=True)
                
                for page_info in pages_data:
                    page_num = page_info.get('page_number', 1)
                    page_text = page_info.get('text', '')
                    page_file = pages_dir / f"page_{page_num:03d}.txt"
                    
                    with open(page_file, 'w', encoding='utf-8') as pf:
                        pf.write(f"–¢–ò–ü –î–û–ö–£–ú–ï–ù–¢–ê: {doc_type}\n")
                        pf.write(f"–§–ê–ô–õ: {image_path_obj.name}\n")
                        pf.write(f"–°–¢–†–ê–ù–ò–¶–ê: {page_num} –∏–∑ {total_pages}\n")
                        pf.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {page_info.get('confidence', 0.0):.2%}\n")
                        pf.write("\n" + "=" * 100 + "\n")
                        pf.write("–¢–ï–ö–°–¢ –°–¢–†–ê–ù–ò–¶–´:\n")
                        pf.write("=" * 100 + "\n")
                        pf.write(page_text)
                        pf.write("\n" + "=" * 100 + "\n")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            result_data = {
                'document_type': doc_type,
                'filename': image_path.name,
                'document_id': result['document_id'],
                'quality': {
                    'overall': result['quality_report']['overall_quality'],
                    'ocr_confidence': result['quality_report']['ocr_confidence'],
                    'text_quality': result['quality_report'].get('text_quality', 0),
                    'image_quality': result['quality_report'].get('image_quality', 0)
                },
                'corrections_count': len(result.get('corrections_applied', [])),
                'text_length': len(result['extracted_data']['full_text']),
                'text_file': str(text_file.relative_to(Path('data'))),
                'extracted_fields': result['extracted_data'].get('fields', {}),
                'validation_results': result.get('validation_results', {}),
                'processing_timestamp': datetime.now().isoformat()
            }
            
            results.append(result_data)
            processed += 1
            
            print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: –∫–∞—á–µ—Å—Ç–≤–æ {result['quality_report']['overall_quality']:.2%}, "
                  f"–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(result.get('corrections_applied', []))}, "
                  f"—Ç–µ–∫—Å—Ç: {len(result['extracted_data']['full_text'])} —Å–∏–º–≤–æ–ª–æ–≤")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {str(e)}")
            errors += 1
            import traceback
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {image_path}: {traceback.format_exc()}")
            continue
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    summary = {
        'total_pairs': len(all_pairs),
        'processed': processed,
        'errors': errors,
        'document_types': doc_types,
        'pairs_by_type': {doc_type: len(loader.find_document_pairs(doc_type)) 
                          for doc_type in doc_types},
        'results': results,
        'processing_timestamp': datetime.now().isoformat()
    }
    
    summary_file = output_dir / "summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if results:
        avg_quality = sum(r['quality']['overall'] for r in results) / len(results)
        avg_ocr_conf = sum(r['quality']['ocr_confidence'] for r in results) / len(results)
        total_corrections = sum(r['corrections_count'] for r in results)
        total_text_length = sum(r['text_length'] for r in results)
        
        print("\n" + "=" * 100)
        print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 100)
        print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}/{len(all_pairs)}")
        print(f"‚ùå –û—à–∏–±–æ–∫: {errors}")
        print(f"\nüìà –°—Ä–µ–¥–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:")
        print(f"   –ö–∞—á–µ—Å—Ç–≤–æ: {avg_quality:.2%}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {avg_ocr_conf:.2%}")
        print(f"   –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {total_corrections} (–≤ —Å—Ä–µ–¥–Ω–µ–º {total_corrections/processed:.1f} –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç)")
        print(f"   –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {total_text_length} —Å–∏–º–≤–æ–ª–æ–≤ (–≤ —Å—Ä–µ–¥–Ω–µ–º {total_text_length/processed:.0f} –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç)")
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   - –¢–µ–∫—Å—Ç—ã: {output_dir}/")
        print(f"   - –°–≤–æ–¥–∫–∞: {summary_file}")
    
    print("\n" + "=" * 100)
    print("–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 100)
    
    return results


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    results = process_dataset_phase1()
    
    if results:
        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: data/dataset_results_phase1/")


if __name__ == "__main__":
    main()
