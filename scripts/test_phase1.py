#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–∞–∑—ã 1: –ë–∞–∑–æ–≤—ã–π OCR + –ø—Ä–∞–≤–∏–ª–∞
–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
"""
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –ø—É—Ç—å
app_root = Path(__file__).parent.parent
if str(app_root) not in sys.path:
    sys.path.insert(0, str(app_root))

from core.processor import DocumentPipeline
import json
import logging
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_single_document(file_path: str, document_type: str = None):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print("\n" + "=" * 80)
    print(f"–û–ë–†–ê–ë–û–¢–ö–ê –î–û–ö–£–ú–ï–ù–¢–ê: {Path(file_path).name}")
    print("=" * 80)
    
    if not Path(file_path).exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return None
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ (—Ç–æ–ª—å–∫–æ –§–∞–∑–∞ 1)
        pipeline = DocumentPipeline(use_ml=False, use_active_learning=False)
        
        print(f"üìÑ –§–∞–π–ª: {file_path}")
        print(f"üìã –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {document_type or '–Ω–µ —É–∫–∞–∑–∞–Ω'}")
        print("\n‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞...")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        result = pipeline.process(
            file_path=file_path,
            template=document_type,
            required_fields=["ogrn", "inn", "date"]
        )
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "-" * 80)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–†–ê–ë–û–¢–ö–ò")
        print("-" * 80)
        
        print(f"\n‚úÖ Document ID: {result['document_id']}")
        print(f"üìÖ –î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['processing_date']}")
        print(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ: {result['quality_report']['overall_quality']:.2%}")
        print(f"üîç –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {result['quality_report'].get('ocr_confidence', 0):.2%}")
        print(f"‚ö†Ô∏è  –¢—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {'–î–∞' if result['needs_review'] else '–ù–µ—Ç'}")
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
        print("\n" + "-" * 80)
        print("–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–û–õ–Ø")
        print("-" * 80)
        critical_fields = result['extracted_data']['critical_fields']
        
        if critical_fields:
            for field_name, field_data in critical_fields.items():
                status = "‚úÖ" if field_data['valid'] else "‚ùå"
                print(f"\n{status} {field_name.upper()}:")
                print(f"   –ó–Ω–∞—á–µ–Ω–∏–µ: {field_data['value'] or '(–Ω–µ –Ω–∞–π–¥–µ–Ω–æ)'}")
                print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {field_data['confidence']:.2%}")
                print(f"   –í–∞–ª–∏–¥–Ω–æ: {'–î–∞' if field_data['valid'] else '–ù–µ—Ç'}")
                if field_data.get('suggested_correction'):
                    print(f"   üí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {field_data['suggested_correction']}")
        else:
            print("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        print("\n" + "-" * 80)
        print("–ü–†–ò–ú–ï–ù–ï–ù–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø")
        print("-" * 80)
        corrections = result.get('corrections_applied', [])
        if corrections:
            for i, correction in enumerate(corrections, 1):
                print(f"{i}. '{correction['from']}' -> '{correction['to']}' "
                      f"(—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {correction['confidence']:.2%}, –º–µ—Ç–æ–¥: {correction.get('method', 'unknown')})")
        else:
            print("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å")
        
        # –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞
        print("\n" + "-" * 80)
        print("–ü–†–û–ë–õ–ï–ú–´ –ö–ê–ß–ï–°–¢–í–ê")
        print("-" * 80)
        issues = result['quality_report'].get('issues', [])
        if issues:
            for issue in issues:
                print(f"‚ö†Ô∏è  {issue.get('type', 'unknown')}: {issue.get('message', '')}")
        else:
            print("‚úÖ –ü—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        
        # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤)
        print("\n" + "-" * 80)
        print("–ü–û–õ–ù–´–ô –¢–ï–ö–°–¢ (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤)")
        print("-" * 80)
        full_text = result['extracted_data']['full_text']
        print(full_text[:300] + ("..." if len(full_text) > 300 else ""))
        
        return result
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def test_multiple_documents(documents_dir: str, document_type: str = None, limit: int = 3):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    print("\n" + "=" * 80)
    print(f"–ü–ê–ö–ï–¢–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í")
    print("=" * 80)
    
    documents_path = Path(documents_dir)
    if not documents_path.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {documents_dir}")
        return
    
    # –ü–æ–∏—Å–∫ PDF –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_files = []
    for ext in ['*.pdf', '*.jpg', '*.jpeg', '*.png']:
        image_files.extend(list(documents_path.glob(ext)))
    
    if not image_files:
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π/PDF –≤ {documents_dir}")
        return
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
    image_files = sorted(image_files)[:limit]
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(image_files)}")
    print(f"üìã –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {document_type or '–Ω–µ —É–∫–∞–∑–∞–Ω'}")
    
    results = []
    for i, file_path in enumerate(image_files, 1):
        print(f"\n[{i}/{len(image_files)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path.name}")
        result = test_single_document(str(file_path), document_type)
        if result:
            results.append(result)
    
    # –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 80)
    print("–°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)
    
    if results:
        avg_quality = sum(r['quality_report']['overall_quality'] for r in results) / len(results)
        total_corrections = sum(len(r.get('corrections_applied', [])) for r in results)
        needs_review_count = sum(1 for r in results if r.get('needs_review', False))
        
        print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(results)}")
        print(f"üìà –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {avg_quality:.2%}")
        print(f"‚úèÔ∏è  –í—Å–µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {total_corrections}")
        print(f"‚ö†Ô∏è  –¢—Ä–µ–±—É—é—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {needs_review_count}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—è–º
        all_fields = {}
        for result in results:
            for field_name, field_data in result['extracted_data']['critical_fields'].items():
                if field_name not in all_fields:
                    all_fields[field_name] = {'found': 0, 'valid': 0, 'total': 0}
                all_fields[field_name]['total'] += 1
                if field_data['value']:
                    all_fields[field_name]['found'] += 1
                if field_data['valid']:
                    all_fields[field_name]['valid'] += 1
        
        if all_fields:
            print("\nüìã –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—è–º:")
            for field_name, stats in all_fields.items():
                found_pct = (stats['found'] / stats['total']) * 100 if stats['total'] > 0 else 0
                valid_pct = (stats['valid'] / stats['total']) * 100 if stats['total'] > 0 else 0
                print(f"   {field_name.upper()}: –Ω–∞–π–¥–µ–Ω–æ {stats['found']}/{stats['total']} ({found_pct:.1f}%), "
                      f"–≤–∞–ª–∏–¥–Ω–æ {stats['valid']}/{stats['total']} ({valid_pct:.1f}%)")
    
    return results


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 80)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –§–ê–ó–´ 1: –ë–ê–ó–û–í–´–ô OCR + –ü–†–ê–í–ò–õ–ê")
    print("=" * 80)
    
    # –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
    dataset_root = Path("../–î–∞—Ç–∞—Å–µ—Ç/–ù–∞–±–æ—Ä—ã –æ–¥–Ω–æ—Ç–∏–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ —Å–∫–∞–Ω–∞–º–∏")
    
    # –¢–µ—Å—Ç 1: –û–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç
    print("\nüî¨ –¢–ï–°–¢ 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    test_file = dataset_root / "–ê–∫—Ç –ê–û–°–†" / "1 –ê–û–°–†.pdf"
    if test_file.exists():
        test_single_document(str(test_file), "–ê–∫—Ç –ê–û–°–†")
    else:
        print(f"‚ùå –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {test_file}")
    
    # –¢–µ—Å—Ç 2: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    print("\n\nüî¨ –¢–ï–°–¢ 2: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    aosr_dir = dataset_root / "–ê–∫—Ç –ê–û–°–†"
    if aosr_dir.exists():
        test_multiple_documents(str(aosr_dir), "–ê–∫—Ç –ê–û–°–†", limit=3)
    else:
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {aosr_dir}")
    
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 80)


if __name__ == "__main__":
    main()
