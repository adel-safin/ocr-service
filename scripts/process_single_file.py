#!/usr/bin/env python3
"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ –≤—Å–µ —Ç—Ä–∏ —Ñ–∞–∑—ã
"""
import sys
from pathlib import Path
import json
from datetime import datetime

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –ø—É—Ç—å
app_root = Path(__file__).parent.parent
if str(app_root) not in sys.path:
    sys.path.insert(0, str(app_root))

from core.processor import DocumentPipeline
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def process_single_file(file_path: str):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ –≤—Å–µ —Ñ–∞–∑—ã"""
    print("=" * 100)
    print("–û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–ê - –í–°–ï –§–ê–ó–´")
    print("=" * 100)
    
    file_path_obj = Path(file_path)
    
    if not file_path_obj.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return
    
    print(f"\nüìÅ –§–∞–π–ª: {file_path_obj.name}")
    print(f"üìã –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ –≤—Å–µ —Ñ–∞–∑—ã...\n")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ñ–∞–∑—ã
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤...")
    pipeline_phase1 = DocumentPipeline(use_ml=False, use_active_learning=False)
    pipeline_phase2 = DocumentPipeline(use_ml=True, use_active_learning=False)
    pipeline_phase3 = DocumentPipeline(use_ml=True, use_active_learning=True)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_dir = Path("data/single_file_results")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    phase1_dir = output_dir / "phase1"
    phase2_dir = output_dir / "phase2"
    phase3_dir = output_dir / "phase3"
    
    phase1_dir.mkdir(exist_ok=True)
    phase2_dir.mkdir(exist_ok=True)
    phase3_dir.mkdir(exist_ok=True)
    
    try:
        # –§–ê–ó–ê 1: –ë–∞–∑–æ–≤—ã–π OCR + –ø—Ä–∞–≤–∏–ª–∞
        print(f"   üìå –§–∞–∑–∞ 1...", end=" ")
        result1 = pipeline_phase1.process(str(file_path_obj), template="certificate")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –§–∞–∑—ã 1
        pages_data = result1['extracted_data'].get('pages', [])
        total_pages = result1['extracted_data'].get('total_pages', 1)
        
        phase1_file = phase1_dir / f"{file_path_obj.stem}_phase1.txt"
        with open(phase1_file, 'w', encoding='utf-8') as f:
            f.write(f"–§–ê–ô–õ: {file_path_obj.name}\n")
            f.write(f"–§–ê–ó–ê 1: –ë–ê–ó–û–í–´–ô OCR + –ü–†–ê–í–ò–õ–ê\n")
            f.write("=" * 100 + "\n")
            f.write(f"Document ID: {result1['document_id']}\n")
            f.write(f"–ö–∞—á–µ—Å—Ç–≤–æ: {result1['quality_report']['overall_quality']:.2%}\n")
            f.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {result1['quality_report']['ocr_confidence']:.2%}\n")
            f.write(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(result1.get('corrections_applied', []))}\n")
            f.write(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}\n")
            
            # –î–µ—Ç–∞–ª–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_report = result1['quality_report']
            f.write(f"\n–î–ï–¢–ê–õ–ò –ö–ê–ß–ï–°–¢–í–ê:\n")
            f.write(f"  - –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {quality_report.get('ocr_confidence', 0):.2%}\n")
            img_quality = quality_report.get('image_quality', {})
            if isinstance(img_quality, dict):
                f.write(f"  - –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_quality.get('overall_quality', 0):.2%}\n")
            else:
                f.write(f"  - –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_quality:.2%}\n")
            f.write(f"  - –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {quality_report.get('overall_quality', 0):.2%}\n")
            
            if quality_report.get('warnings'):
                f.write(f"\n‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:\n")
                for warning in quality_report['warnings']:
                    f.write(f"  - {warning}\n")
            
            f.write("\n" + "=" * 100 + "\n")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            if pages_data and len(pages_data) > 1:
                f.write("–†–ê–°–ü–û–ó–ù–ê–ù–ù–´–ô –¢–ï–ö–°–¢ –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú:\n")
                f.write("=" * 100 + "\n")
                for page_info in pages_data:
                    page_num = page_info.get('page_number', 1)
                    page_text = page_info.get('text', '')
                    page_conf = page_info.get('confidence', 0.0)
                    f.write(f"\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {page_conf:.2%}) ---\n")
                    f.write(page_text)
                    f.write("\n")
            else:
                f.write("–ü–û–õ–ù–´–ô –¢–ï–ö–°–¢:\n")
                f.write("=" * 100 + "\n")
                f.write(result1['extracted_data']['full_text'])
                f.write("\n" + "=" * 100 + "\n")
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            if result1.get('corrections_applied'):
                f.write("\n–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:\n")
                f.write("-" * 100 + "\n")
                for j, correction in enumerate(result1['corrections_applied'], 1):
                    f.write(f"{j}. {correction.get('from', '')} -> {correction.get('to', '')} (–º–µ—Ç–æ–¥: {correction.get('method', 'unknown')})\n")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if pages_data and len(pages_data) > 1:
            pages_dir = phase1_dir / f"{file_path_obj.stem}_pages"
            pages_dir.mkdir(exist_ok=True)
            
            for page_info in pages_data:
                page_num = page_info.get('page_number', 1)
                page_text = page_info.get('text', '')
                page_file = pages_dir / f"page_{page_num:03d}.txt"
                
                with open(page_file, 'w', encoding='utf-8') as pf:
                    pf.write(f"–§–ê–ô–õ: {file_path_obj.name}\n")
                    pf.write(f"–§–ê–ó–ê 1: –ë–ê–ó–û–í–´–ô OCR + –ü–†–ê–í–ò–õ–ê\n")
                    pf.write(f"–°–¢–†–ê–ù–ò–¶–ê: {page_num} –∏–∑ {total_pages}\n")
                    pf.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {page_info.get('confidence', 0.0):.2%}\n")
                    pf.write("\n" + "=" * 100 + "\n")
                    pf.write("–¢–ï–ö–°–¢ –°–¢–†–ê–ù–ò–¶–´:\n")
                    pf.write("=" * 100 + "\n")
                    pf.write(page_text)
                    pf.write("\n" + "=" * 100 + "\n")
        
        print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ: {result1['quality_report']['overall_quality']:.2%}")
        
        # –§–ê–ó–ê 2: –° ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
        print(f"   ü§ñ –§–∞–∑–∞ 2...", end=" ")
        result2 = pipeline_phase2.process(str(file_path_obj), template="certificate")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –§–∞–∑—ã 2
        pages_data2 = result2['extracted_data'].get('pages', [])
        total_pages2 = result2['extracted_data'].get('total_pages', 1)
        
        phase2_file = phase2_dir / f"{file_path_obj.stem}_phase2.txt"
        with open(phase2_file, 'w', encoding='utf-8') as f:
            f.write(f"–§–ê–ô–õ: {file_path_obj.name}\n")
            f.write(f"–§–ê–ó–ê 2: –ú–ê–®–ò–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï\n")
            f.write("=" * 100 + "\n")
            f.write(f"Document ID: {result2['document_id']}\n")
            f.write(f"–ö–∞—á–µ—Å—Ç–≤–æ: {result2['quality_report']['overall_quality']:.2%}\n")
            f.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {result2['quality_report']['ocr_confidence']:.2%}\n")
            f.write(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(result2.get('corrections_applied', []))}\n")
            f.write(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages2}\n")
            
            # –î–µ—Ç–∞–ª–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_report = result2['quality_report']
            f.write(f"\n–î–ï–¢–ê–õ–ò –ö–ê–ß–ï–°–¢–í–ê:\n")
            f.write(f"  - –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {quality_report.get('ocr_confidence', 0):.2%}\n")
            img_quality = quality_report.get('image_quality', {})
            if isinstance(img_quality, dict):
                f.write(f"  - –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_quality.get('overall_quality', 0):.2%}\n")
            else:
                f.write(f"  - –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_quality:.2%}\n")
            f.write(f"  - –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {quality_report.get('overall_quality', 0):.2%}\n")
            
            if quality_report.get('warnings'):
                f.write(f"\n‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:\n")
                for warning in quality_report['warnings']:
                    f.write(f"  - {warning}\n")
            
            f.write("\n" + "=" * 100 + "\n")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            if pages_data2 and len(pages_data2) > 1:
                f.write("–†–ê–°–ü–û–ó–ù–ê–ù–ù–´–ô –¢–ï–ö–°–¢ –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú:\n")
                f.write("=" * 100 + "\n")
                for page_info in pages_data2:
                    page_num = page_info.get('page_number', 1)
                    page_text = page_info.get('text', '')
                    page_conf = page_info.get('confidence', 0.0)
                    f.write(f"\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {page_conf:.2%}) ---\n")
                    f.write(page_text)
                    f.write("\n")
            else:
                f.write("–ü–û–õ–ù–´–ô –¢–ï–ö–°–¢:\n")
                f.write("=" * 100 + "\n")
                f.write(result2['extracted_data']['full_text'])
                f.write("\n" + "=" * 100 + "\n")
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            if result2.get('corrections_applied'):
                f.write("\n–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:\n")
                f.write("-" * 100 + "\n")
                for j, correction in enumerate(result2['corrections_applied'], 1):
                    f.write(f"{j}. {correction.get('from', '')} -> {correction.get('to', '')} (–º–µ—Ç–æ–¥: {correction.get('method', 'unknown')})\n")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if pages_data2 and len(pages_data2) > 1:
            pages_dir = phase2_dir / f"{file_path_obj.stem}_pages"
            pages_dir.mkdir(exist_ok=True)
            
            for page_info in pages_data2:
                page_num = page_info.get('page_number', 1)
                page_text = page_info.get('text', '')
                page_file = pages_dir / f"page_{page_num:03d}.txt"
                
                with open(page_file, 'w', encoding='utf-8') as pf:
                    pf.write(f"–§–ê–ô–õ: {file_path_obj.name}\n")
                    pf.write(f"–§–ê–ó–ê 2: –ú–ê–®–ò–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï\n")
                    pf.write(f"–°–¢–†–ê–ù–ò–¶–ê: {page_num} –∏–∑ {total_pages2}\n")
                    pf.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {page_info.get('confidence', 0.0):.2%}\n")
                    pf.write("\n" + "=" * 100 + "\n")
                    pf.write("–¢–ï–ö–°–¢ –°–¢–†–ê–ù–ò–¶–´:\n")
                    pf.write("=" * 100 + "\n")
                    pf.write(page_text)
                    pf.write("\n" + "=" * 100 + "\n")
        
        print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ: {result2['quality_report']['overall_quality']:.2%}")
        
        # –§–ê–ó–ê 3: –° –∞–∫—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º
        print(f"   üîÑ –§–∞–∑–∞ 3...", end=" ")
        result3 = pipeline_phase3.process(str(file_path_obj), template="certificate")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –§–∞–∑—ã 3
        pages_data3 = result3['extracted_data'].get('pages', [])
        total_pages3 = result3['extracted_data'].get('total_pages', 1)
        
        phase3_file = phase3_dir / f"{file_path_obj.stem}_phase3.txt"
        with open(phase3_file, 'w', encoding='utf-8') as f:
            f.write(f"–§–ê–ô–õ: {file_path_obj.name}\n")
            f.write(f"–§–ê–ó–ê 3: –ê–ö–¢–ò–í–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï\n")
            f.write("=" * 100 + "\n")
            f.write(f"Document ID: {result3['document_id']}\n")
            f.write(f"–ö–∞—á–µ—Å—Ç–≤–æ: {result3['quality_report']['overall_quality']:.2%}\n")
            f.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {result3['quality_report']['ocr_confidence']:.2%}\n")
            f.write(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(result3.get('corrections_applied', []))}\n")
            f.write(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages3}\n")
            
            # –î–µ—Ç–∞–ª–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_report = result3['quality_report']
            f.write(f"\n–î–ï–¢–ê–õ–ò –ö–ê–ß–ï–°–¢–í–ê:\n")
            f.write(f"  - –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {quality_report.get('ocr_confidence', 0):.2%}\n")
            img_quality = quality_report.get('image_quality', {})
            if isinstance(img_quality, dict):
                f.write(f"  - –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_quality.get('overall_quality', 0):.2%}\n")
            else:
                f.write(f"  - –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_quality:.2%}\n")
            f.write(f"  - –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {quality_report.get('overall_quality', 0):.2%}\n")
            
            if quality_report.get('warnings'):
                f.write(f"\n‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:\n")
                for warning in quality_report['warnings']:
                    f.write(f"  - {warning}\n")
            
            f.write("\n" + "=" * 100 + "\n")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            if pages_data3 and len(pages_data3) > 1:
                f.write("–†–ê–°–ü–û–ó–ù–ê–ù–ù–´–ô –¢–ï–ö–°–¢ –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú:\n")
                f.write("=" * 100 + "\n")
                for page_info in pages_data3:
                    page_num = page_info.get('page_number', 1)
                    page_text = page_info.get('text', '')
                    page_conf = page_info.get('confidence', 0.0)
                    f.write(f"\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {page_conf:.2%}) ---\n")
                    f.write(page_text)
                    f.write("\n")
            else:
                f.write("–ü–û–õ–ù–´–ô –¢–ï–ö–°–¢:\n")
                f.write("=" * 100 + "\n")
                f.write(result3['extracted_data']['full_text'])
                f.write("\n" + "=" * 100 + "\n")
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            if result3.get('corrections_applied'):
                f.write("\n–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:\n")
                f.write("-" * 100 + "\n")
                for j, correction in enumerate(result3['corrections_applied'], 1):
                    f.write(f"{j}. {correction.get('from', '')} -> {correction.get('to', '')} (–º–µ—Ç–æ–¥: {correction.get('method', 'unknown')})\n")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if pages_data3 and len(pages_data3) > 1:
            pages_dir = phase3_dir / f"{file_path_obj.stem}_pages"
            pages_dir.mkdir(exist_ok=True)
            
            for page_info in pages_data3:
                page_num = page_info.get('page_number', 1)
                page_text = page_info.get('text', '')
                page_file = pages_dir / f"page_{page_num:03d}.txt"
                
                with open(page_file, 'w', encoding='utf-8') as pf:
                    pf.write(f"–§–ê–ô–õ: {file_path_obj.name}\n")
                    pf.write(f"–§–ê–ó–ê 3: –ê–ö–¢–ò–í–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï\n")
                    pf.write(f"–°–¢–†–ê–ù–ò–¶–ê: {page_num} –∏–∑ {total_pages3}\n")
                    pf.write(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {page_info.get('confidence', 0.0):.2%}\n")
                    pf.write("\n" + "=" * 100 + "\n")
                    pf.write("–¢–ï–ö–°–¢ –°–¢–†–ê–ù–ò–¶–´:\n")
                    pf.write("=" * 100 + "\n")
                    pf.write(page_text)
                    pf.write("\n" + "=" * 100 + "\n")
        
        print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ: {result3['quality_report']['overall_quality']:.2%}")
        
        print(f"\n   ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ\n")
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   - –§–∞–∑–∞ 1: {phase1_file}")
        print(f"   - –§–∞–∑–∞ 2: {phase2_file}")
        print(f"   - –§–∞–∑–∞ 3: {phase3_file}")
        
        if pages_data and len(pages_data) > 1:
            print(f"   - –°—Ç—Ä–∞–Ω–∏—Ü—ã –§–∞–∑—ã 1: {phase1_dir / f'{file_path_obj.stem}_pages'}/")
        if pages_data2 and len(pages_data2) > 1:
            print(f"   - –°—Ç—Ä–∞–Ω–∏—Ü—ã –§–∞–∑—ã 2: {phase2_dir / f'{file_path_obj.stem}_pages'}/")
        if pages_data3 and len(pages_data3) > 1:
            print(f"   - –°—Ç—Ä–∞–Ω–∏—Ü—ã –§–∞–∑—ã 3: {phase3_dir / f'{file_path_obj.stem}_pages'}/")
        
        print("\n" + "=" * 100)
        print("–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print("=" * 100)
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {str(e)}\n")
        import traceback
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path_obj}: {traceback.format_exc()}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python process_single_file.py <–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É>")
        print("–ü—Ä–∏–º–µ—Ä: python process_single_file.py '../—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã/29-52 –ü–ê–°–ü–û–†–¢–ê .pdf'")
        return
    
    file_path = sys.argv[1]
    process_single_file(file_path)


if __name__ == "__main__":
    main()

