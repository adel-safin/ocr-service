#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
"""
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –ø—É—Ç—å
app_root = Path(__file__).parent.parent
if str(app_root) not in sys.path:
    sys.path.insert(0, str(app_root))

from core.processor import DocumentPipeline
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_classifier():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
    print("=" * 80)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ë–£–ß–ï–ù–ù–û–ì–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å ML
    pipeline = DocumentPipeline(use_ml=True, use_active_learning=False)
    
    if not pipeline.document_classifier:
        print("\n‚ùå –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω!")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/train_classifier.py")
        return
    
    print("\n‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    if hasattr(pipeline, 'class_mapping'):
        classes = pipeline.class_mapping.get('class_to_idx', {})
        print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(classes)}")
        print(f"   –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
        for i, doc_type in enumerate(list(classes.keys())[:10], 1):
            print(f"      {i}. {doc_type}")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –î–û–ö–£–ú–ï–ù–¢–ê–•")
    print("=" * 80)
    
    test_files = [
        "../–î–∞—Ç–∞—Å–µ—Ç/–ù–∞–±–æ—Ä—ã –æ–¥–Ω–æ—Ç–∏–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ —Å–∫–∞–Ω–∞–º–∏/–ê–∫—Ç –ê–û–°–†/1 –ê–û–°–†.pdf",
        "../–î–∞—Ç–∞—Å–µ—Ç/–ù–∞–±–æ—Ä—ã –æ–¥–Ω–æ—Ç–∏–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ —Å–∫–∞–Ω–∞–º–∏/–ê–∫—Ç –≤–Ω–µ—à–Ω–µ–≥–æ –æ—Å–º–æ—Ç—Ä–∞/1 –ê–ö–¢.pdf",
        "../–î–∞—Ç–∞—Å–µ—Ç/–ù–∞–±–æ—Ä—ã –æ–¥–Ω–æ—Ç–∏–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ —Å–∫–∞–Ω–∞–º–∏/–ê–∫—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è/1 –ê–í–ö.jpg",
    ]
    
    for test_file in test_files:
        if not Path(test_file).exists():
            continue
        
        print(f"\nüìÑ –§–∞–π–ª: {Path(test_file).name}")
        
        try:
            device = 'cuda' if hasattr(__import__('torch'), 'cuda') and __import__('torch').cuda.is_available() else 'cpu'
            doc_type_idx, confidence = pipeline.document_classifier.predict(test_file, device=device)
            
            if hasattr(pipeline, 'class_mapping'):
                predicted_type = pipeline.class_mapping['idx_to_class'].get(doc_type_idx, 'unknown')
                print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–∏–ø: {predicted_type}")
                print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏
                expected_type = Path(test_file).parent.name
                is_correct = predicted_type == expected_type
                status = "‚úÖ" if is_correct else "‚ùå"
                print(f"   {status} –û–∂–∏–¥–∞–ª—Å—è: {expected_type}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {str(e)}")
    
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 80)


if __name__ == "__main__":
    test_classifier()
