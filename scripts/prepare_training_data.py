#!/usr/bin/env python3
"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
"""
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –ø—É—Ç—å
app_root = Path(__file__).parent.parent
if str(app_root) not in sys.path:
    sys.path.insert(0, str(app_root))

from utils.dataset_loader import DatasetLoader
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def prepare_classifier_data():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
    print("\n" + "=" * 80)
    print("–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê –î–û–ö–£–ú–ï–ù–¢–û–í")
    print("=" * 80)
    
    dataset_root = Path("../–î–∞—Ç–∞—Å–µ—Ç")
    loader = DatasetLoader(str(dataset_root))
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–∞—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    all_pairs = []
    doc_types = loader.get_all_document_types()
    
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_types)}")
    
    for doc_type in doc_types:
        pairs = loader.find_document_pairs(doc_type)
        all_pairs.extend(pairs)
        print(f"   {doc_type}: {len(pairs)} –ø–∞—Ä")
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ –ø–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(all_pairs)}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞–Ω–Ω—ã—Ö
    training_info = {
        'total_pairs': len(all_pairs),
        'document_types': doc_types,
        'pairs_by_type': {doc_type: len(loader.find_document_pairs(doc_type)) 
                          for doc_type in doc_types}
    }
    
    output_path = Path("data/training_data/classifier_info.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(training_info, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
    
    return all_pairs, doc_types


def prepare_spell_correction_data():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫"""
    print("\n" + "=" * 80)
    print("–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –û–ü–ï–ß–ê–¢–û–ö")
    print("=" * 80)
    
    dataset_root = Path("../–î–∞—Ç–∞—Å–µ—Ç")
    loader = DatasetLoader(str(dataset_root))
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ + —ç—Ç–∞–ª–æ–Ω)
    pairs = loader.find_document_pairs()
    
    print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(pairs)}")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä (OCR —Ç–µ–∫—Å—Ç -> —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç)
    from core.ocr_engine import OCREngine
    
    ocr_engine = OCREngine()
    training_pairs = []
    
    print("\n‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    
    for i, pair in enumerate(pairs[:20], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∞
        print(f"   [{i}/{min(20, len(pairs))}] {Path(pair['image_path']).name}")
        
        try:
            # OCR —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
            ocr_result = ocr_engine.process_file(pair['image_path'])
            ocr_text = ocr_result['text']
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            reference_text = loader.load_reference_text(pair['reference_path'])
            
            if ocr_text.strip() and reference_text.strip():
                training_pairs.append({
                    'ocr_text': ocr_text[:500],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                    'reference_text': reference_text[:500],
                    'document_type': pair['document_type']
                })
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pair['image_path']}: {str(e)}")
    
    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(training_pairs)}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    output_path = Path("data/training_data/spell_correction_pairs.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(training_pairs, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
    
    return training_pairs


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 80)
    print("–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø ML –ú–û–î–ï–õ–ï–ô")
    print("=" * 80)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    pairs, doc_types = prepare_classifier_data()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫
    spell_pairs = prepare_spell_correction_data()
    
    print("\n" + "=" * 80)
    print("–ü–û–î–ì–û–¢–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 80)
    print("\nüìä –ò—Ç–æ–≥–∏:")
    print(f"   - –ü–∞—Ä –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {len(pairs)}")
    print(f"   - –¢–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_types)}")
    print(f"   - –ü–∞—Ä –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫: {len(spell_pairs)}")
    print("\nüí° –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("   1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/train_classifier.py")
    print("   2. –î–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫ –Ω—É–∂–Ω–∞ –¥–æ–æ–±—É—á–∫–∞ T5 (–±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞)")


if __name__ == "__main__":
    main()
